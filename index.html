<!doctype html>
<html>
    <head>
        <meta charset="utf-8">

        <title>Revisiting k-means: New Algorithms via Bayesian Nonparametrics </title>

        <meta name="description" content="SP2020">
        <meta name="author" content="Aytijhya Saha">

        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

        <meta name="viewport" content="width=device-width, initial-scale=1.0">

        <link rel="stylesheet" href="dist/reset.css">
        <link rel="stylesheet" href="dist/reveal.css">
        <link rel="stylesheet" href="dist/theme/night.css" id="theme">

        <!-- Theme used for syntax highlighting of code -->
        <link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
        <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"> -->
        <link href="css/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
        <link href="css/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
    </head>
    <body>
        <div class="reveal">
            <div class="slides">
                <section data-background="network2.gif" data-background-opacity=0.2 style="font-size: 33px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
                    <h3 id="heading" style="color: white; text-align: center; margin-bottom: 0.8cm; font-size: 60px;">
                        <b>Revisiting k-means: New Algorithms via Bayesian Nonparametrics</b>
                	</h3>

					
                    <hr>
                    <div style="text-align: center; margin-top: 1.5cm;">
                        A presentation by:
                        <br>
					</div>
					<div style="text-align: center; margin-top: 0.5cm;">
                        <strong>Aytijhya Saha</strong>
                        <br>
                        B.Stat 3rd Year<br>
						Indian Statistical Institute, Kolkata
                    </div>
					<div style="text-align: center; margin-top: 0.8cm;">
                        March 20, 2023
                        <br>
					</div>
                </section>

                <section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
                    <h4>
                        Paper Description
                    </h4>
                    
                    <div>
						<ul>
							<li class="fragment">
								Paper title: Revisiting k-means: New Algorithms via Bayesian Nonparametrics
							</li>
						</ul>
					</div>

					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Authors: Brian Kulis (Department of CSE, Ohio State University, Columbus),
                                 Michael I. Jordan (Departments of EECS and Statistics, University of California, Berkeley)
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Published at International Conference on Machine Learning (ICML), 2012
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Number of Citations: 441
							</li>
						</ul>
					</div>
					<hr>
					
				</section>

			<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Introduction <br>  
					</font> 
				</h4>
			    <section>
                    <h3>
						Statistical Learning/ Machine Learning  
                    </h3>
					
					<div>
						Machine Learning is a set of methods that can automatically detect patterns in data, and then use the uncovered patterns to 
						predict future data, or to perform other kinds of decision-making under uncertainty.
					</div>
                    
			
                </section>

				<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
                    <h3>
                        What is cluster analysis?
                    </h3>
					<div>
						
					</div>
					<div>

                    Clustering is an unsupervised method for partitioning a collection of data
                    points, into groups, such that the objects in a group will be similar (or related) to one another and different from (or unrelated to) the objects in other groups.
					</div>
                    
			
                </section>

				<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
                    <h4>
					Traditional Methods of Clustering
                    </h4>
                    <div>
						<ul>
							<li class="fragment">
								K-means clustering \cite{macqueen1967classification}
							</li>
						</ul>
					</div>

					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Hierarchial Clustering \cite{ward1963hierarchical}
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Spectral Clustering \cite{guattery1994performance}
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Fuzzy Clustering \cite{dunn1973fuzzy}
							</li>
							
						</ul>
						
					</div>
					<div style="margin-top: 0.6cm;">
						And so on....
					</div>
					<hr>
					
                </section>
            </section>

			<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Background <br>  
					</font> 
				</h4>
			    <section>
                    <h4>
                        K-means clustering alogorithm
                    </h4>
                    
                    <div>
						<ul>
							<li class="fragment">
								Decide on a value for the number of clusters, $k$.
							</li>
						</ul>
					</div>

					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Initialize the $k$ cluster centers (randomly).
							</li>
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Decide the class memberships of the $N$ objects by assigning them to the nearest cluster center.	
							</li>
						
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								Re-estimate the $k$ cluster centers, using the mean of each cluster.	
							</li>
						
						</ul>
					</div>
					<div style="margin-top: 0.6cm;">
						<ul>
							<li class="fragment">
								If none of the N objects changed membership in the last iteration, exit. Otherwise goto 3.	
							</li>
							
						</ul>
						
					</div>
					
					<hr>
				    </section>

					<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
						<h3>
						K-means objective function
						</h3>
						
						<div>
						Note that, given a set of data points $\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n}$, the k-means algorithm attempts to find clusters $\ell_{1}, \ldots, \ell_{k}$ 
						to minimize the following objective function:
						$$
						\min_{\left\{\ell_{c}\right\}_{c=1}^{k}} \sum_{c=1}^{k} \sum_{\boldsymbol{x} \in \ell_{c}}\left\|\boldsymbol{x}-\boldsymbol{\mu}_{c}\right\|_2^2
						\text { , where }  \boldsymbol{\mu}_{c}=\dfrac{1}{\left|\ell_{c}\right|} \sum_{\boldsymbol{x} \in \ell_{c}} \boldsymbol{x} .
						$$
						</div>
						
				
					</section>	
                
					<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
						<h3>
							Connection between EM algorithm for GMM and K-means Algorithm
						</h3>
						
						<div>
						Suppose in the mixture of Gaussians model that all Gaussians have the same fixed covariance equal to $\sigma I$. Because they are fixed, the covariances need not be re-estimated during the M-step. In this case, the E-step takes the following form:
						$$
						\gamma\left(z_{i c}\right)=\frac{\pi_{c} \cdot \exp \left(-\frac{1}{2 \sigma}\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right\|_{2}^{2}\right)}{\sum_{j=1}^{k} \pi_{j} \cdot \exp \left(-\frac{1}{2 \sigma}\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\right\|_{2}^{2}\right)},
						$$ 
						here $\gamma\left(z_{i c}\right)$ is the probability of assigning point $i$ to cluster $c$.

						Observe that as $\sigma \rightarrow 0$, $\gamma\left(z_{i c}\right)\to 1$, for $c=\arg \min \left\|\boldsymbol{x}_{i}-\mu_{c}\right\|_{2}^{2}$ and $\gamma\left(z_{i c}\right)\to 0$, otherwise.
						This implies that the E-step is equivalent to the reassignment step of k-means, and one can further easily show that the M-step exactly recomputes the means of the new clusters, establishing the equivalence of the updates.

						</div>
					
					</section>

					<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
						<h3>
							Connection between EM algorithm for GMM and K-means Algorithm
						</h3>
						
						<div class="fragment">
							The k-means algorithm may be viewed as a limit of the expectation - maximization (EM) algorithm. 
							If all of the covariance matrices corresponding to the clusters in a Gaussian mixture model 
							are equal to $\sigma I$ and $\sigma \to 0$, the EM steps approach the $\mathrm{k}$-means steps in the limit.
						</div>
						<div class="fragment">	
							K-means is straightforward
							to implement and works well for a variety of applications.
							But, a major drawback of this algorithm is that the value of k needs to be specified by the user, which is not known in prior.
						</div>
						<div class="fragment">	
						Whereas, Bayesian nonparametric models, viz. the Dirichlet process mixture \cite{hjort2010bayesian}, result in infinite mixture models which do not
						fix the number of clusters in the data upfront; these methods
						continue to gain popularity in the statistical learning community.
					    </div>
					</section>

					<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
						<h3>
							Connection between EM algorithm for GMM and K-means Algorithm
						</h3>
						
						<div class="fragment">
							A Bayesian extension to the mixture model arises by first placing a Dirichlet prior,
							$\operatorname{Dir}\left(k, \boldsymbol{\pi}_{0}\right)$, on the mixing coefficients,
							for some $\boldsymbol{\pi}_{0}$. If we further assume that the covariances of the Gaussians
							are fixed to $\sigma I$ and that the means are drawn from some prior distribution $G_{0}$,
						    we obtain the following Bayesian model:

						</div>
						<div class="fragment">
							$$
							\begin{aligned}
							\boldsymbol{\mu}_{1}, \ldots, \boldsymbol{\mu}_{k} & \sim G_{0} \\
							\boldsymbol{\pi} & \sim \operatorname{Dir}\left(k, (\alpha / k) \boldsymbol{e}\right) \\
							\boldsymbol{z}_{1}, \ldots, \boldsymbol{z}_{n} & \sim \operatorname{Discrete}(\boldsymbol{\pi}) \\
							\boldsymbol{x}_{1}, \ldots, \boldsymbol{x}_{n} & \sim \mathbf{N}\left(\boldsymbol{\mu}_{\boldsymbol{z}_{i}}, \sigma I\right)
							\end{aligned}
							$$
						</div>
						<div>
							Dirichlet process mixture model is the above model, as $k \to \infty$.
						</div>
					
					</section>
				</section>

                <section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Hard Clustering via Dirichlet Process <br>  
					</font> 
				</h4>
				    <section>
						<h4>
						DP Gibbs Sampler 
						</h4>
						<div class="fragment">
							A simplest algorithm for inference in a DP mixture is based on Gibbs sampling:

						</div>

						<div>
							<ul>
								<li class="fragment">
									Let the state of the underlying Markov chain
									consists of the set of all cluster indicators and the set of
									all cluster means.
								</li>
							</ul>
						</div>
	
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									The algorithm proceeds by first looping
									repeatedly through each of the data points and performing
									Gibbs moves on the cluster indicators for each point.
								</li>
							</ul>
						</div>
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									For $i=1, \ldots, n$, we reassign $\boldsymbol{x}_{i}$ to existing cluster $c$ with probability $\frac{n_{-i, c}}{Z} \cdot \mathcal{N}\left(\boldsymbol{x}_{i} \mid \boldsymbol{\mu}_{c}, \sigma I\right)$, where $n_{-i, c}$ is the number of data points (excluding $\boldsymbol{x}_{i}$) that are assigned to cluster $c$. With probability
									$
									\frac{\alpha}{Z} \int \mathcal{N}\left(\boldsymbol{x}_{i} \mid \boldsymbol{\mu}, \sigma I\right) d G_{0}(\boldsymbol{\mu})
									$,
									we start a new cluster. $Z$ is an appropriate normalizing constant here.

                                    If we end up choosing to start a new cluster, we select its mean from the posterior distribution obtained from the prior $G_{0}$ and the single sample $\boldsymbol{x}_{i}$. 
								</li>
							
							</ul>
						</div>
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									Next, we perform Gibbs moves on the means: sample $\boldsymbol{\mu}_{c}$ from the posterior based on prior $G_0$ and all points currently assigned to cluster $c, \forall c$.
								</li>
							
							</ul>
						</div>
						
					</section>

					<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
							<h3>
								Connection between EM algorithm for GMM and K-means Algorithm
							</h3>
							
							<div>
								Then, we have the following probabilities to be
								used during Gibbs sampling:
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										The posterior probability
										of point $i$ being assigned to cluster $c$,
										$$\hat{\gamma}\left(z_{i c}\right)=\frac{n_{-i, c} \cdot \exp \left(-\frac{1}{2 \sigma}\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{c}\right\|^{2}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\left\|\boldsymbol{x}_{i}\right\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\right\|^{2}\right)}$$
									</li>
								
								</ul>
							</div>

							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
								     and the posterior probability of starting a new cluster
                                     $$\hat{\gamma}\left(z_{i, n e w}\right)=\frac{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\left\|\boldsymbol{x}_{i}\right\|^{2}}{2(\rho+\sigma)}\right)}{\exp \left(-\frac{\lambda}{2 \sigma}-\frac{\left\|\boldsymbol{x}_{i}\right\|^{2}}{2(\rho+\sigma)}\right)+\sum_{j=1}^{k} n_{-i, j} \cdot \exp \left(-\frac{1}{2 \sigma}\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{j}\right\|^{2}\right)}.$$ 
									</li>
								
								</ul>
							</div>
						
					</section>

					<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
							<h4>
								Asymptotics of the DP Gibbs Sampler
							</h4>
							
							<div>
								<ul>
									<li class="fragment">
										As $\sigma \rightarrow 0$, the values of $\hat{\gamma}\left(z_{i, c}\right)$ and $\hat{\gamma}\left(z_{i, n e w}\right)$ will be increasingly dominated by the smallest value of $\left\{\left\|\boldsymbol{x}_{i}-\boldsymbol{\mu}_{1}\right\|^{2}, \ldots, \| \boldsymbol{x}_{i}-\boldsymbol{\mu}_{k} \|^{2}, \lambda\right\}$ and only the smallest of these values will receive a non-zero $\hat{\gamma}$ value.
									</li>
								</ul>
							</div>
		
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										This implies reassignment a point to the cluster corresponding to the closest mean, if the closest cluster has squared distance smaller than $\lambda$. Otherwise, we start a new cluster, in which case we sample a new mean from the posterior based on the prior $G_{0}$ and  single observation 
									</li>
								</ul>
							</div>
							<div style="margin-top: 0.6cm;">
								<ul>
									<li class="fragment">
										Similarly, once we have performed Gibbs moves on the cluster assignments, we must perform Gibbs moves on all the means, which amounts to sampling from the posterior based on $G_{0}$ and all observations in a cluster.
									</li>
								
								</ul>
							</div>
							
						
					</section>
                    <section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
							
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									Since the prior and likelihood are Gaussian, the posterior will be Gaussian as well. If we let $\overline{\boldsymbol{x}}_{c}$ be the mean of the points currently assigned to cluster $c$ and $n_{c}$ be the number of points assigned to cluster $c$, then the
									posterior is a Gaussian with mean $\tilde{\boldsymbol{\mu}}_{c}$ and covariance $\tilde{\Sigma}_{c}$, where
									$
									\tilde{\boldsymbol{\mu}}_{c}=\left(1+\frac{\sigma}{\rho n_{c}}\right)^{-1} \overline{\boldsymbol{x}}_{c}, \quad \tilde{\Sigma}_{c}=\frac{\sigma \rho}{\sigma+\rho n_{c}} I
									$.
								</li>
							
							</ul>
						</div>
						<div style="margin-top: 0.6cm;">
							<ul>
								<li class="fragment">
									As $\sigma \rightarrow 0$, $\tilde{\boldsymbol{\mu}}_{c}\to \overline{\boldsymbol{x}}_{c}$ and $\tilde{\Sigma}_{c}\to \mathbf{0}$,
									meaning that the mass of the distribution becomes concentrated at $\overline{\boldsymbol{x}}_{c}$. Thus, in the limit we choose $\overline{\boldsymbol{x}}_{c}$ as the mean,
									which leads us to:
								</li>
							
							</ul>
						</div>
						
						</section>
		
					</section>

				<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
						data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				
					<section>
						<h3>
							<font color="#FFFF00">  
								Algorithm 1 : DP-means <br>  
								</font> 
							
						</h3>
						
						<div class="fragment">
							<font color="#FFFF01">  
								Input :
							</font>
							$\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n$ : input data, $\lambda:$ cluster penalty parameter
						</div>
						<div class="fragment">
							<font color="#FFFF01">  
								Output:
							</font>
							Clustering $\ell_1, \ldots, \ell_k$ and number of clusters $k$
						</div>
						<div class="fragment">
							<ul>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 1.
									</font> 
									Initialize: $k=1, \ell_1=\left\{\boldsymbol{x}_1, \ldots, \boldsymbol{x}_n\right\}$ and $\boldsymbol{\mu}_1$ the global mean.
								</li>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 2.
									</font> 
									Initialize: cluster indicators $z_i=1$ for all $i=1, \ldots, n$.
								</li>
								<li class="fragment">
									<font color="#FFFF01">  
										Step 3.
									</font> 
									Repeat until convergence:
									<ol>
										<li>
											For each point $\boldsymbol{x}_i$, compute $d_{i c}=\left\|\boldsymbol{x}_i-\boldsymbol{\mu}_c\right\|^2$ for $c=1, \ldots, k$
										</li>

										<li>
											If $\min _c d_{i c}>\lambda$, set $k=k+1, z_i=k$, and $\boldsymbol{\mu}_k=\boldsymbol{x}_i$.
										\item Otherwise, set $z_i=\operatorname{argmin}_c d_{i c}$.

										</li>
										<li>
											Generate clusters $\ell_1, \ldots, \ell_k$ based on $z_1, \ldots, z_k: \ell_j=$ $\left\{\boldsymbol{x}_i \mid z_i=j\right\}$
										</li>
										<li>
											For each cluster $\ell_j$, compute $\boldsymbol{\mu}_j=\frac{1}{\left|\ell_j\right|} \sum_{\boldsymbol{x} \in \ell_j} \boldsymbol{x}$.
										</li>
									</ol>

								</li>
							</div>
							
					</section>

					<section>

						<h3>
							<font color="#FFFF00">  
								Convergence of Objective function of DP means <br>  
							</font> 
							
						</h3>
							<div class="fragment">
							
								<font color="#FFFF00">  
									Theorem: 
								</font>  DP means algorithm  monotonically decreases the following objective until local convergence:
								$$
								\begin{array}{cc}
								\min _{\left\{\ell_{c}\right\}_{c=1}^{k}} & \sum_{c=1}^{k} \sum_{\boldsymbol{x} \in \ell_{c}}\left\|\boldsymbol{x}-\boldsymbol{\mu}_{c}\right\|^{2}+\lambda k \\
								\text { where } & \boldsymbol{\mu}_{c}=\frac{1}{\left|\ell_{c}\right|} \sum_{\boldsymbol{x} \in \ell_{c}} \boldsymbol{x} .
								\end{array}
								$$	
							
							</div>
							<div class="fragment">
							
								<font color="#FFFF00">  
									Proof:
								</font>
									The reassignment step in the algorithm results in a non-increasing objective since the distance between
									 a point and its newly assigned cluster mean never increases; for distances greater than $\lambda$,
									  we can generate a new cluster and pay a penalty of $\lambda$ while still decreasing the objective.
									 Similarly, the mean update step results in a non-increasing objective since the mean is the best representative of a cluster in terms of the squared Euclidean distance.
									  The fact that the algorithm will converge locally follows from the fact that the objective function cannot increase,
									and that there are only a finite number of possible clusterings of the data.
							</div>
					</section>	
				</section>
 

				<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;" data-auto-animate
				data-auto-animate-easing="cubic-bezier(0.770, 0.000, 0.175, 1.000)">
				<h4>
					<font color="#FFFF00">  
						Experimental Results <br>  
					</font> 
				</h4>

				    <section>
					<div class="fragment">
						To demonstrate
						comparable accuracies among the methods- k-means, DP-means, and Gibbs sampling, 8
						common UCI data sets were selected.
						The NMI (normalized
						mutual information) is computed between the ground-truth and the computed clusters, and results are averaged over 10 runs. The
						results are shown in the following table:
					
					</div>

					<div class="fragment">
						<table>
							<tr>
								<td>Dataset</td>
								<td>DP Means</td>
								<td>K-means</td>
								<td>Gibbs</td>
							</tr>
							<tr>
								<td>Soybean</td>
								<td>.72</td>
								<td>.66</td>
								<td>.73</td>
							</tr>
							<tr>
								<td>Wine</td>
								<td>.41</td>
								<td>.43</td>
								<td>.42</td>
							</tr>
							<tr>
								<td>Breast Cancer</td>
								<td>.04</td>
								<td>.03</td>
								<td>.04</td>
							</tr>
							<tr>
								<td>Car</td>
								<td>.07</td>
								<td>.05</td>
								<td>.15</td>
							</tr>
							<tr>
								<td>Balance Scale</td>
								<td>.17</td>
								<td>.11</td>
								<td>.14</td>
							</tr>
							<tr>
								<td>Vehicle</td>
								<td>.18</td>
								<td>.18</td>
								<td>.17</td>
							</tr>
							<tr>
								<td>Iris</td>
								<td>.75</td>
								<td>.76</td>
								<td>.73</td>
							</tr>
							<tr>
								<td>Pima</td>
								<td>.02</td>
								<td>.03</td>
								<td>.02</td>
							</tr>
							<hr>
						</table>
					</div>
				    </section>
				</section>
				<section data-background="bg.jpeg" data-background-opacity=0.4 style="text-align: left; font-size: 26px;">
                    <h4>
						<font color="#FFFF00">  
						Conclusion <br>  
						</font> 
					</h4>
                    This paper outlines
					connections arising between DP mixture models and hard
					clustering algorithms, and develops scalable algorithms, like k-means for
					hard clustering that is capable of retaining some of the benefits of Bayesian
					nonparametric. This algorithm do not require to specify the number of clusters,
					 which is a major advantage over k-means. But, it has some disadvantages also, for example,
					the method involves a hyperparameter and sometimes it is difficult to tune the hyperpapameter.
                    
					
				</section>
				
				<section data-background="network2.gif" data-background-opacity=0.2 style="font-size: 33px;" >
                    <h3 style="color: rgb(11, 202, 245); text-align: center; margin-bottom: 0.8cm; font-size: 60px;">
                        <strong>Thank you</strong>
                    </h3>
				</section>
				
            </div>
        </div>
        <script src="dist/reveal.js"></script>
        <script src="plugin/notes/notes.js"></script>
        <script src="plugin/markdown/markdown.js"></script>
        <script src="plugin/zoom/zoom.js"></script>
        <script src="plugin/highlight/highlight.js"></script>
        <script src="plugin/math/math.js"></script>
        <script src="plugin/chalkboard/plugin.js"></script>
        <script>
            // More info about initialization & config:
            // - https://revealjs.com/initialization/
            // - https://revealjs.com/config/
            Reveal.initialize({
                hash: true,
                transition: 'concave',
				backgroundTransition: 'slide',
                // controls: false,
				controls: true,

                // Display a presentation progress bar
                progress: true,

                // Push each slide change to the browser history
                history: false,

                // Enable keyboard shortcuts for navigation
                keyboard: true,

				slideNumber: true,

				// Can be used to limit the contexts in which the slide number appears
				// - "all":      Always show the slide number
				// - "print":    Only when printing to PDF
				// - "speaker":  Only in the speaker view
				showSlideNumber: 'all',				

                // Loop the presentation
                loop: false,

                // Number of milliseconds between automatically proceeding to the 
                // next slide, disabled when set to 0
                autoSlide: 0,

                // Enable slide navigation via mouse wheel
                mouseWheel: false,

                // Apply a 3D roll to links on hover
                rollingLinks: true,
                // Learn about plugins: https://revealjs.com/plugins/
                // chalkboard: {
                    // src: "chalkboard/chalkboard.json",
                    // toggleChalkboardButton: false,
                    // toggleNotesButton: false,
                // },
                plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath ],
            });
        </script>
    </body>
</html>

